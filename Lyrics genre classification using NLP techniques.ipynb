{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b265315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4018e7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18514 entries, 0 to 18513\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Genre   18514 non-null  object\n",
      " 1   Lyrics  18514 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 289.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7936 entries, 0 to 7935\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Genre   7936 non-null   object\n",
      " 1   Lyrics  7936 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 124.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('Lyrics-Genre-Train.csv', usecols = [3, 4], names = ['Genre', 'Lyrics'])\n",
    "train_data.info()\n",
    "\n",
    "test_data = pd.read_csv('Lyrics-Genre-Test-GroundTruth.csv', usecols = [3, 4], names = ['Genre', 'Lyrics'])\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "52083d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, X_valid, Y_valid, X_test, Y_test, model_name):\n",
    "    \n",
    "    #compute accuracy for the validation dataset\n",
    "    print(model_name + \" accuracy score on the validation dataset: \", accuracy_score(Y_valid, model.predict(X_valid)))\n",
    "\n",
    "    #compute accuracy for the test dataset\n",
    "    print(model_name + \" accuracy score on the test dataset: \", accuracy_score(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "12fad67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def run_models(X_train, Y_train, X_valid, Y_valid, X_test, Y_test, model_name):\n",
    "    svm_mdl = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    svm_mdl.fit(X_train, Y_train)\n",
    "    compute_accuracy(svm_mdl, X_valid, Y_valid, X_test, Y_test, model_name + \" with SVM\")\n",
    "    \n",
    "    rf_mdl = RandomForestClassifier(max_depth = 15, n_estimators = 350, random_state = 1)\n",
    "    rf_mdl.fit(X_train, Y_train)\n",
    "    compute_accuracy(rf_mdl, X_valid, Y_valid, X_test, Y_test, model_name + \" with Random Forest\")\n",
    "\n",
    "    nb_mdl = GaussianNB()\n",
    "    nb_mdl.fit(X_train, Y_train)\n",
    "    compute_accuracy(nb_mdl, X_valid, Y_valid, X_test, Y_test, model_name + \" with Naive Bayes\")\n",
    "\n",
    "    nn_mdl = MLPClassifier(random_state = 1, activation = \"relu\", solver = \"adam\", hidden_layer_sizes=(50,50, 25), max_iter = 300)\n",
    "    nn_mdl.fit(X_train, Y_train)\n",
    "    compute_accuracy(nb_mdl, X_valid, Y_valid, X_test, Y_test, model_name + \" with Neural Networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392b02f",
   "metadata": {},
   "source": [
    "<h2>Bag of words and SVM model - the baseline</h2>\n",
    "\n",
    "The first approach used in order to classify the lyrics by genre is to use the classical Bag of words (BOW) technique togheter with the Support Vector Machine model. BOW is a natural language processing technique used to represent a colection of documents by the word present in it. All the words present in all the documents of the colection represents the vocabulary of known words; every document is represented as a list of numbers with the dimension equals with the number of the words from the vocabulary. So every number from the list represents the number of occurances of the i-th word (from the vocabulary) in the current document.\n",
    "\n",
    "The input from the 'Lyrics' column is tokenized by words, the stopwords are removed, also the punctuation, all characters are normalized and a stemmer is applied to the resulted text. The next step is to pass this preprocessed input to the BOW model.The obtained output will be passed to the SVM model. Our purpoise is to see if the results obtained using BOW can be overtaken by other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "865fad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(data):\n",
    "    #tokenize the lyrics into words\n",
    "    data['tknz_lyrics'] = data['Lyrics'].apply(lambda row: word_tokenize(row))\n",
    "\n",
    "     # remove all characters that are not letters\n",
    "    all_lyrics = []\n",
    "    for lyric in data['tknz_lyrics']:\n",
    "        current_lyric = []\n",
    "        for word in lyric:\n",
    "            if word.isalpha():\n",
    "                current_lyric.append(word.lower())\n",
    "        all_lyrics.append(current_lyric)\n",
    "    \n",
    "    \n",
    "    # remove the stop words (words that are not necesar in the meaning of a phrase)\n",
    "    stop_words_eng = set(stopwords.words('english'))\n",
    "    without_stop_words = []\n",
    "    for lyric in all_lyrics:\n",
    "        current_lyric = []\n",
    "        for word in lyric:\n",
    "            if word not in stop_words_eng:\n",
    "                current_lyric.append(word)\n",
    "        without_stop_words.append(current_lyric)\n",
    "    \n",
    "    #perform stemming step in order to truncate words to its root form \n",
    "    snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    stemmed_lyrics = []\n",
    "    for lyric in without_stop_words:\n",
    "        aux_lyric = []\n",
    "        for word in lyric:\n",
    "            aux_lyric.append(snowball_stemmer.stem(str(word)))\n",
    "        stemmed_lyrics.append(aux_lyric)\n",
    "        \n",
    "    data['prepross_lyrics'] = stemmed_lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a07710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text(train_data)\n",
    "preprocess_text(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff63eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18514, 4)\n",
      "(7936, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_data))\n",
    "print(np.shape(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3736caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Bag of words algorithm over the tokenized lyrics\n",
    "def bow(training, test): \n",
    "    count_vect = CountVectorizer(max_features = 10000)\n",
    "    count_data = count_vect.fit(training + test)\n",
    "    training_out = count_vect.transform(training)\n",
    "    test_out = count_vect.transform(test)\n",
    "    return (training_out.toarray(), test_out.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbcf56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18514, 10000)\n",
      "(7936, 10000)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data_for_BOW():\n",
    "    #prepare the data obtained by the preprocessing step in order to be passed to the model\n",
    "    X_train = train_data['prepross_lyrics'].tolist()\n",
    "    X_test = test_data['prepross_lyrics'].tolist()\n",
    "\n",
    "    Y_train = train_data['Genre'].tolist()\n",
    "    Y_test = test_data['Genre'].tolist()\n",
    "\n",
    "    X_train =[\" \".join(x) for x in X_train]\n",
    "    X_test =[\" \".join(x) for x in X_test]\n",
    "\n",
    "    X_train, X_test = bow(X_train, X_test)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a02fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data_for_BOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bea89b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the training dataset into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba59c76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "svm_mdl = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svm_mdl.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80eaa33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW and SVM accuracy score on the validation dataset:  0.31163921145017554\n",
      "BOW and SVM accuracy score on the test dataset:  0.30934979838709675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#compute accuracy for the validation dataset\n",
    "print(\"BOW and SVM accuracy score on the validation dataset: \", accuracy_score(Y_valid, svm_mdl.predict(X_valid)))\n",
    "      \n",
    "#compute accuracy for the test dataset\n",
    "print(\"BOW and SVM accuracy score on the test dataset: \", accuracy_score(Y_test, svm_mdl.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ac624",
   "metadata": {},
   "source": [
    "<h2>TF-IDF method</h2>\n",
    "\n",
    "Try the term frequencyâ€“inverse document frequency (TF-IDF) is a NLP method that has the role to find out how important is a word for a document. For every word of every document, a weight-factor is applied, so every word will receive a score computed on the frequency of that word in the corpus and offseted by the number of the documents from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b277c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raradu\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\raradu\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\raradu\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7936, 10000)\n",
      "(14811, 10000)\n",
      "(3703, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#apply td-idf to the bow method\n",
    "tfidf_obj = TfidfTransformer(smooth_idf=False)\n",
    "tfidf_test = tfidf_obj.fit_transform(X_test).toarray()\n",
    "tfidf_train = tfidf_obj.fit_transform(X_train).toarray()\n",
    "tfidf_valid = tfidf_obj.fit_transform(X_valid).toarray()\n",
    "print(np.shape(tfidf_test))\n",
    "print(np.shape(tfidf_train))\n",
    "print(np.shape(tfidf_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23f2c52",
   "metadata": {},
   "source": [
    "The TF-IDF input will be passed to SVM, Random Forest, Naive Bayes and Neural Networks models in order to see on what model we can obtain a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "89fd84dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW and TD-IDF with SVM accuracy score on the validation dataset:  0.33864434242506075\n",
      "BOW and TD-IDF with SVM accuracy score on the test dataset:  0.33845766129032256\n",
      "BOW and TD-IDF with Random Forest accuracy score on the validation dataset:  0.33648393194706994\n",
      "BOW and TD-IDF with Random Forest accuracy score on the test dataset:  0.3286290322580645\n",
      "BOW and TD-IDF with Naive Bayes accuracy score on the validation dataset:  0.26357007831487983\n",
      "BOW and TD-IDF with Naive Bayes accuracy score on the test dataset:  0.26764112903225806\n",
      "BOW and TD-IDF with Neural Networks accuracy score on the validation dataset:  0.3219011612206319\n",
      "BOW and TD-IDF with Neural Networks accuracy score on the test dataset:  0.3186743951612903\n"
     ]
    }
   ],
   "source": [
    "run_models(tfidf_train, Y_train, tfidf_valid, Y_valid, tfidf_test, Y_test, \"BOW and TD-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a953eb1",
   "metadata": {},
   "source": [
    "<b>Observation: </b>It seems that the TF-IDF method applied to the BOW input performs a little better than the simple BOW applied to the Lyrics input column. The results are not significant better, but SVM and Random Forest seems to perform a little better than the baseline (BOW and SVM).\n",
    "\n",
    "In the above table, it can be observed all the obtained results for the BOW and TF-IDF methods:\n",
    "\n",
    "| No.| Model |Validation Dataset Accuracy | Test Dataset Accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| 1. | SVM | 0.3386 | 0.3384 |\n",
    "| 2. | Random Forest | 0.3364 | 0.3286 |\n",
    "| 3. | Naive Bayes | 0.2635 | 0.2676 |\n",
    "| 4. | Neural Network | 0.3219 | 0.3186 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f618fef",
   "metadata": {},
   "source": [
    "<h2>Word embeddings</h2>\n",
    "\n",
    "After we tried the BOW model, it's time to move on to the next NLP technique that may lead to a better accuracy score. Now, the word embeddings method will be used. In word embeddings, the purpose is to transform every document (i.e. lyric of a song) in a vector of lenght N of real numbers. The difference between BOW vectors and word ebeddings vectors is that the vectors resulted in word embeddings contain some more information than the BOW vectors did. In word embeddings, the vector of a word can be closer or far to anoter word (in the sense of a distance); do this distance can say if some words are used togheter or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c20be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for word embeddings\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "def preprocess_text_for_wemb(data):\n",
    "    #tokenize the lyrics into words\n",
    "    data['tknz_lyrics'] = data['Lyrics'].apply(lambda row: word_tokenize(row))\n",
    "\n",
    "     # remove all characters that are not letters\n",
    "    all_lyrics = []\n",
    "    for lyric in data['tknz_lyrics']:\n",
    "        current_lyric = []\n",
    "        for word in lyric:\n",
    "            if word.isalpha():\n",
    "                current_lyric.append(word.lower())\n",
    "        all_lyrics.append(current_lyric)\n",
    "    \n",
    "    \n",
    "    # remove the stop words (words that are not necesar in the meaning of a phrase)\n",
    "    stop_words_eng = set(stopwords.words('english'))\n",
    "    without_stop_words = []\n",
    "    for lyric in all_lyrics:\n",
    "        current_lyric = []\n",
    "        for word in lyric:\n",
    "            if word not in stop_words_eng:\n",
    "                current_lyric.append(word)\n",
    "        without_stop_words.append(current_lyric)\n",
    "    \n",
    "    #perform stemming step in order to truncate words to its root form \n",
    "    snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    stemmed_lyrics = []\n",
    "    for lyric in without_stop_words:\n",
    "        aux_lyric = []\n",
    "        for word in lyric:\n",
    "            aux_lyric.append(snowball_stemmer.stem(str(word)))\n",
    "        stemmed_lyrics.append(aux_lyric)\n",
    "    \n",
    "    detokenized_lyrics = []\n",
    "    for crnt in stemmed_lyrics:\n",
    "        detokenized_lyrics.append(TreebankWordDetokenizer().detokenize(crnt))\n",
    "        \n",
    "    data['wemb_lyrics'] = detokenized_lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bde45f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7936, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>tknz_lyrics</th>\n",
       "      <th>wemb_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genre</td>\n",
       "      <td>Lyrics</td>\n",
       "      <td>[Lyrics]</td>\n",
       "      <td>lyric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Most folks spend their days daydreaming of fin...</td>\n",
       "      <td>[Most, folks, spend, their, days, daydreaming,...</td>\n",
       "      <td>folk spend day daydream find clue whole life t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indie</td>\n",
       "      <td>Take your cold hands and put them on my face\\n...</td>\n",
       "      <td>[Take, your, cold, hands, and, put, them, on, ...</td>\n",
       "      <td>take cold hand put face sharpen axe crimin way...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Genre                                             Lyrics  \\\n",
       "0    Genre                                             Lyrics   \n",
       "1  Hip-Hop  Most folks spend their days daydreaming of fin...   \n",
       "2    Indie  Take your cold hands and put them on my face\\n...   \n",
       "\n",
       "                                         tknz_lyrics  \\\n",
       "0                                           [Lyrics]   \n",
       "1  [Most, folks, spend, their, days, daydreaming,...   \n",
       "2  [Take, your, cold, hands, and, put, them, on, ...   \n",
       "\n",
       "                                         wemb_lyrics  \n",
       "0                                              lyric  \n",
       "1  folk spend day daydream find clue whole life t...  \n",
       "2  take cold hand put face sharpen axe crimin way...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text_for_wemb(train_data)\n",
    "preprocess_text_for_wemb(test_data)\n",
    "print(np.shape(train_data))\n",
    "print(np.shape(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc4432b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "def create_vects(text):\n",
    "    doc = nlp(text)\n",
    "    vector = doc.vector\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8763cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7935\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "X_wemb_train = []\n",
    "for index, row in train_data.iterrows():\n",
    "    X_wemb_train.append(create_vects(row['wemb_lyrics']))\n",
    "\n",
    "X_wemb_test = []\n",
    "for index, row in test_data.iterrows():\n",
    "    X_wemb_test.append(create_vects(row['wemb_lyrics']))\n",
    "\n",
    "Y_wemb_train = train_data['Genre']\n",
    "Y_wemb_test = test_data['Genre']\n",
    "#split the training dataset into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_wemb_train, X_wemb_valid, Y_train_wemb, Y_wemb_valid = train_test_split(X_wemb_train, Y_wemb_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "960a9589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings with SVM accuracy score on the validation dataset:  0.4080475290305158\n",
      "Word embeddings with SVM accuracy score on the test dataset:  0.3978074596774194\n",
      "Word embeddings with Random Forest accuracy score on the validation dataset:  0.3610586011342155\n",
      "Word embeddings with Random Forest accuracy score on the test dataset:  0.3584929435483871\n",
      "Word embeddings with Naive Bayes accuracy score on the validation dataset:  0.30812854442344045\n",
      "Word embeddings with Naive Bayes accuracy score on the test dataset:  0.30279737903225806\n",
      "Word embeddings with Neural Networks accuracy score on the validation dataset:  0.32298136645962733\n",
      "Word embeddings with Neural Networks accuracy score on the test dataset:  0.32497479838709675\n"
     ]
    }
   ],
   "source": [
    "run_models(X_wemb_train, Y_train_wemb, X_wemb_valid, Y_wemb_valid, X_wemb_test, Y_wemb_test, \"Word Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96171b55",
   "metadata": {},
   "source": [
    "<b>Observation: </b> It seems that the word embeddings technique performs better than BOW and BOW togheter with TF-IDF. For all the tried models, the accuracy score is better, but the best results were obtained for the SVN model.\n",
    "\n",
    "In the above table, it can be observed all the obtained results for the word embeddings method:\n",
    "\n",
    "| No.| Model |Validation Dataset Accuracy | Test Dataset Accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| 1. | SVM | 0.4080 | 0.3978 |\n",
    "| 2. | Random Forest | 0.3610 | 0.3584 |\n",
    "| 3. | Naive Bayes | 0.3081 | 0.3027 |\n",
    "| 4. | Neural Network | 0.3229 | 0.3249 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17620afa",
   "metadata": {},
   "source": [
    "After the clasical methods were applied, we will try to optimize our results by adding additional features to our model.\n",
    "\n",
    "<h3>Remove the records that contain non-english lyrics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b14ea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "train_data.name = 'train'\n",
    "test_data.name = 'test'\n",
    "print(train_data.name)\n",
    "print(test_data.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebf65089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cy in train\n",
      "Found fi in train\n",
      "Found es in train\n",
      "Found cy in train\n",
      "Found it in train\n",
      "Found fr in train\n",
      "Found hr in train\n",
      "Found fr in train\n",
      "Found fr in train\n",
      "Found sw in train\n",
      "Found et in train\n",
      "Found so in train\n",
      "Found cy in train\n",
      "Found sl in train\n",
      "Found es in train\n",
      "Found tr in train\n",
      "Found cy in test\n",
      "Found cy in test\n",
      "Found sw in test\n",
      "Found af in test\n",
      "Found cy in test\n",
      "Found cy in test\n",
      "Found nl in test\n"
     ]
    }
   ],
   "source": [
    "def create_only_english_data(data):\n",
    "    eng_data_list = []\n",
    "    non_eng_cnt = 0\n",
    "    for index, row in data.iterrows():\n",
    "        det_lang = detect(row['Lyrics'])\n",
    "        if det_lang == 'en':\n",
    "            crnt_row = row.to_dict()\n",
    "            eng_data_list.append(crnt_row)\n",
    "        else:\n",
    "            non_eng_cnt += 1\n",
    "            print (\"Found \" + det_lang + \" in \" + data.name)\n",
    "    eng_data = pd.DataFrame(eng_data_list, columns = ['Genre', 'Lyrics'])\n",
    "    return non_eng_cnt, eng_data\n",
    "\n",
    "train_non_eng, eng_train_data = create_only_english_data(train_data)\n",
    "test_non_eng, eng_test_data = create_only_english_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb58685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18498 entries, 0 to 18497\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Genre   18498 non-null  object\n",
      " 1   Lyrics  18498 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 289.2+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7929 entries, 0 to 7928\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Genre   7929 non-null   object\n",
      " 1   Lyrics  7929 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 124.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(eng_train_data.info())\n",
    "print(eng_test_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "402bde53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEgCAYAAABYaaN4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdA0lEQVR4nO3de7hcVZ3m8e9LwBAbYnM5IJ0EEjE6HZghSMwTxJkGaU2EwSDTSNIKaUWDNHJpbXuC3SOiE4fu9jKD3TAT5RJsBMPjhXCVEFHEicQDHQgBMkQIEJKHRGw1iEYTf/PHXpXsHOpcknPOqqLW+3meeqr2qtq1fic5561da6+9tyICMzMrwx6tLsDMzPJx6JuZFcShb2ZWEIe+mVlBHPpmZgVx6JuZFWTPVhfQnwMPPDDGjx/f6jLMzF5RHnjggZ9GRFfP9rYP/fHjx9Pd3d3qMszMXlEkPd2s3cM7ZmYFceibmRXEoW9mVhCHvplZQRz6ZmYFceibmRXEoW9mVhCHvplZQdr+4Kz+jJ9326DfY+1lJw9BJWZm7c9b+mZmBXHom5kVxKFvZlYQh76ZWUEc+mZmBXHom5kVxKFvZlYQh76ZWUEc+mZmBXHom5kVxKFvZlYQh76ZWUEc+mZmBXHom5kVpN/Ql7S3pOWSHpK0StKlqX1/SUskPZHu96utc7GkNZJWS5peaz9G0sr03OWSNDw/lpmZNTOQLf0twNsi4ihgMjBD0jRgHrA0IiYCS9MykiYBs4AjgBnAFZJGpPe6EpgLTEy3GUP3o5iZWX/6Df2ovJgW90q3AGYCC1P7QuDU9HgmcGNEbImIp4A1wFRJhwCjI2JZRARwXW0dMzPLYEBj+pJGSFoBbASWRMT9wMERsQEg3R+UXj4GeLa2+rrUNiY97tluZmaZDCj0I2JbREwGxlJttR/Zx8ubjdNHH+0vfwNprqRuSd2bNm0aSIlmZjYAuzR7JyJ+DnyPaiz++TRkQ7rfmF62DhhXW20ssD61j23S3qyfBRExJSKmdHV17UqJZmbWh4HM3umS9Ifp8SjgT4HHgcXAnPSyOcDN6fFiYJakkZImUO2wXZ6GgDZLmpZm7ZxVW8fMzDLYcwCvOQRYmGbg7AEsiohbJS0DFkk6G3gGOB0gIlZJWgQ8CmwFzouIbem9zgWuBUYBd6SbmZll0m/oR8TDwNFN2l8ATuxlnfnA/Cbt3UBf+wPMzGwY+YhcM7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OC7NnqAjrF+Hm3DWr9tZedPESVmJn1rt8tfUnjJN0j6TFJqyRdmNo/Jek5SSvS7aTaOhdLWiNptaTptfZjJK1Mz10uScPzY5mZWTMD2dLfCnwsIh6UtC/wgKQl6bkvRsTn6i+WNAmYBRwB/BFwt6Q3RMQ24EpgLvAj4HZgBnDH0PwoZmbWn3639CNiQ0Q8mB5vBh4DxvSxykzgxojYEhFPAWuAqZIOAUZHxLKICOA64NTB/gBmZjZwu7QjV9J44Gjg/tT0EUkPS7pa0n6pbQzwbG21daltTHrcs93MzDIZcOhL2gf4BnBRRPySaqjmcGAysAH4fOOlTVaPPtqb9TVXUrek7k2bNg20RDMz68eAQl/SXlSBf31EfBMgIp6PiG0R8Xvgy8DU9PJ1wLja6mOB9al9bJP2l4mIBRExJSKmdHV17crPY2ZmfRjI7B0BVwGPRcQXau2H1F72buCR9HgxMEvSSEkTgInA8ojYAGyWNC2951nAzUP0c5iZ2QAMZPbOccCZwEpJK1LbJ4DZkiZTDdGsBc4BiIhVkhYBj1LN/DkvzdwBOBe4FhhFNWvHM3fMzDLqN/Qj4j6aj8ff3sc684H5Tdq7gSN3pUAzMxs6Pg2DmVlBHPpmZgVx6JuZFcShb2ZWEIe+mVlBHPpmZgVx6JuZFcShb2ZWEIe+mVlBHPpmZgVx6JuZFcQXRu8gg704O/gC7Wadzlv6ZmYFceibmRXEoW9mVhCHvplZQRz6ZmYFceibmRXEoW9mVhCHvplZQRz6ZmYFceibmRXEoW9mVpB+Q1/SOEn3SHpM0ipJF6b2/SUtkfREut+vts7FktZIWi1peq39GEkr03OXS9Lw/FhmZtbMQLb0twIfi4g/BqYB50maBMwDlkbERGBpWiY9Nws4ApgBXCFpRHqvK4G5wMR0mzGEP4uZmfWj39CPiA0R8WB6vBl4DBgDzAQWppctBE5Nj2cCN0bEloh4ClgDTJV0CDA6IpZFRADX1dYxM7MMdmlMX9J44GjgfuDgiNgA1QcDcFB62Rjg2dpq61LbmPS4Z7uZmWUy4NCXtA/wDeCiiPhlXy9t0hZ9tDfra66kbkndmzZtGmiJZmbWjwGFvqS9qAL/+oj4Zmp+Pg3ZkO43pvZ1wLja6mOB9al9bJP2l4mIBRExJSKmdHV1DfRnMTOzfgxk9o6Aq4DHIuILtacWA3PS4znAzbX2WZJGSppAtcN2eRoC2ixpWnrPs2rrmJlZBgO5XOJxwJnASkkrUtsngMuARZLOBp4BTgeIiFWSFgGPUs38OS8itqX1zgWuBUYBd6SbmZll0m/oR8R9NB+PBzixl3XmA/ObtHcDR+5KgWZmNnR8RK6ZWUEc+mZmBRnImL7ZLhk/77ZBrb/2spNbXsNQ1WHWbrylb2ZWEIe+mVlBHPpmZgVx6JuZFcShb2ZWEIe+mVlBHPpmZgVx6JuZFcShb2ZWEIe+mVlBHPpmZgVx6JuZFcShb2ZWEIe+mVlBHPpmZgVx6JuZFcShb2ZWEIe+mVlBHPpmZgXpN/QlXS1po6RHam2fkvScpBXpdlLtuYslrZG0WtL0Wvsxklam5y6XpKH/cczMrC8D2dK/FpjRpP2LETE53W4HkDQJmAUckda5QtKI9PorgbnAxHRr9p5mZjaM9uzvBRFxr6TxA3y/mcCNEbEFeErSGmCqpLXA6IhYBiDpOuBU4I7dKdrslWD8vNsG/R5rLzu5Y+qw9jCYMf2PSHo4Df/sl9rGAM/WXrMutY1Jj3u2m5lZRrsb+lcChwOTgQ3A51N7s3H66KO9KUlzJXVL6t60adNulmhmZj3tVuhHxPMRsS0ifg98GZianloHjKu9dCywPrWPbdLe2/sviIgpETGlq6trd0o0M7Mmdiv0JR1SW3w30JjZsxiYJWmkpAlUO2yXR8QGYLOkaWnWzlnAzYOo28zMdkO/O3Il3QAcDxwoaR1wCXC8pMlUQzRrgXMAImKVpEXAo8BW4LyI2Jbe6lyqmUCjqHbgeieumVlmA5m9M7tJ81V9vH4+ML9Jezdw5C5VZ2ZmQ8pH5JqZFcShb2ZWEIe+mVlBHPpmZgVx6JuZFcShb2ZWEIe+mVlBHPpmZgVx6JuZFcShb2ZWEIe+mVlBHPpmZgVx6JuZFcShb2ZWEIe+mVlB+j2fvpnZYI2fd9ug32PtZSd3TB2t5C19M7OCeEvfzCyjVn/b8Ja+mVlBHPpmZgVx6JuZFcShb2ZWEIe+mVlB+g19SVdL2ijpkVrb/pKWSHoi3e9Xe+5iSWskrZY0vdZ+jKSV6bnLJWnofxwzM+vLQLb0rwVm9GibByyNiInA0rSMpEnALOCItM4Vkkakda4E5gIT063ne5qZ2TDrN/Qj4l7gZz2aZwIL0+OFwKm19hsjYktEPAWsAaZKOgQYHRHLIiKA62rrmJlZJrs7pn9wRGwASPcHpfYxwLO1161LbWPS457tZmaW0VDvyG02Th99tDd/E2mupG5J3Zs2bRqy4szMSre7of98GrIh3W9M7euAcbXXjQXWp/axTdqbiogFETElIqZ0dXXtZolmZtbT7ob+YmBOejwHuLnWPkvSSEkTqHbYLk9DQJslTUuzds6qrWNmZpn0e8I1STcAxwMHSloHXAJcBiySdDbwDHA6QESskrQIeBTYCpwXEdvSW51LNRNoFHBHupmZWUb9hn5EzO7lqRN7ef18YH6T9m7gyF2qzszMhpSPyDUzK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIIMKfUlrJa2UtEJSd2rbX9ISSU+k+/1qr79Y0hpJqyVNH2zxZma2a4ZiS/+EiJgcEVPS8jxgaURMBJamZSRNAmYBRwAzgCskjRiC/s3MbICGY3hnJrAwPV4InFprvzEitkTEU8AaYOow9G9mZr0YbOgHcJekByTNTW0HR8QGgHR/UGofAzxbW3ddajMzs0z2HOT6x0XEekkHAUskPd7Ha9WkLZq+sPoAmQtw6KGHDrJEMzNrGNSWfkSsT/cbgW9RDdc8L+kQgHS/Mb18HTCutvpYYH0v77sgIqZExJSurq7BlGhmZjW7HfqS/kDSvo3HwDuAR4DFwJz0sjnAzenxYmCWpJGSJgATgeW727+Zme26wQzvHAx8S1Ljfb4WEXdK+jGwSNLZwDPA6QARsUrSIuBRYCtwXkRsG1T1Zma2S3Y79CPiSeCoJu0vACf2ss58YP7u9mlmZoPjI3LNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysIA59M7OCOPTNzAri0DczK4hD38ysINlDX9IMSaslrZE0L3f/ZmYlyxr6kkYA/wy8E5gEzJY0KWcNZmYly72lPxVYExFPRsRvgRuBmZlrMDMrVu7QHwM8W1tel9rMzCwDRUS+zqTTgekR8cG0fCYwNSLO7/G6ucDctPhGYPUguj0Q+Okg1h8q7VBHO9QA7VFHO9QA7VFHO9QA7VFHO9QAQ1PHYRHR1bNxz0G+6a5aB4yrLY8F1vd8UUQsABYMRYeSuiNiylC81yu9jnaooV3qaIca2qWOdqihXepohxqGu47cwzs/BiZKmiDpVcAsYHHmGszMipV1Sz8itkr6CPAdYARwdUSsylmDmVnJcg/vEBG3A7dn7HJIhomGQDvU0Q41QHvU0Q41QHvU0Q41QHvU0Q41wDDWkXVHrpmZtZZPw2BmVhCHvplZQToy9CWNb9L25haUYm1E0v6trqEdSPqJpA/3aLu1VfW0C0l/0OoacujI0Ae+KWn7kb6S/gS4uhWFSHqtpHdJOkXSa1tUw5skXSDpfElvytz3LZIW93bLWQtwv6SbJJ0kSZn73k7SeZL+sLa8n6S/zFjC74ATJF2Tpk5D5iPjJR0o6ZL0e7mPpCslPSLpZkmvz1zLWyQ9CjyWlo+SdEXOGlK/Zzdpu2yo++nU0D8H+HYK3JOA/wWclLsISR8ElgOnAX8G/EjSBzLX8ElgIXAA1VF+10j6u4wlfA74fB+3nN5ANSviTGCNpM9KekPmGgA+FBE/byxExL8BH8rY/0sRcQZVyP1A0mFA7hkdXwNGAhOp/kaepPobuRX4SuZavghMB14AiIiHgP+UuQaAP5P03sZC+uB52RG1gxYRHXkDjgUepvqF6mpRDauBA2rLBwCrM9fwGLB3bXkU8Fir/39afQNOAJ4Dfg58Hzg2Y98Pk2bOpeURwKqM/f9r7fGJwOPAxsz//g+lewHP9HhuReZa7m/y7/JQzhpSn6OAJcBs4Drgfw5HP9nn6Q8nSbew8xbLq4FfAFdJIiLelbmkdcDm2vJmdj7hXA5rgb2B36TlkcBPMteApInA/6A6pfbejfaIeF3GGg4A3ke1pf88cD7VEeGTgZuACZlK+Q6wSNL/pvp9/TBwZ6a+AT7ZeBARSyVNB+Zk7B9gW+o/JPU8x8zvM9fyrKS3AJGGuy4gDfXk0GNf0weBbwM/BD4taf+I+NlQ9tdRoU81lNBOnqMaR76Z6o97JrBc0kcBIuILGWrYAqyStCTV8HbgPkmXpxouyFADwDXAJVRfpU8A3k+1lZfTMuCrwKkRsa7W3p0COJf/SjUEeS7Vv8Fd5B3SuEjStqgOlCQinpY0NmP/AK9L+3RUe0xazvXh2/BhqiHgMVQbancB52Xs/wGqv03V7k9OtwCGdMOoIw/OkjQB2BARv0nLo4CDI2Jt5jou6ev5iLg0Qw19bsFFxMLhriHV8UBEHCNpZUT8+9T2g4j4j5n6HwH8Y0R8NEd/7UzSk1TfOL/b+B2U9GBEZNvJnyZX9Coivp+rltJ02pZ+w03AW2rL21Jb1mmbtT+ofavFeDFn/6mGhekra2OH5eqI+F3uOoDfSNoDeCKdf+k54KBcnUfENklH5eqvGUmLIuI9klbSZMdpRPyHTKX8nGos//I0JPq+TP1u11eop2G4YSfpbyLiHyR9ieb/H1m+BUs6ra/nI+KbQ9lfp4b+nlFdmQuAiPhtbWpaNpKOpBpO2D8t/xQ4KzKeZE7S8VSzd9ZSfW0cJ2lORNybq4bkIqp9LBcAn6Ea4jkrcw0r0jDCTcCvGo1D/UfVhwvT/X/O1F9vFBFbgb+U9BfAfcB+LS1I+glwG/AvwLVU+36GW2PcvjtDX305pY/nAnDoD8AmSe+KiMUAkmbSmgsjLAA+GhH3pDqOB77Mzt9ChtvngXdExOpUwxuAG4BjMtYAMD4ifgy8SDWe37iozv0Za9ifalre22ptQ/5H1ZuI2JDun87RXx+277+IiGvTN4+cY9gvExGHS/orqv0u78/U5y3pPssQZx91ZPl5Gzp1TP9w4Hp2HHDyLHBmRGSdtSLpoYg4qr+2Ya7h4Z7DBs3aMtTxsjHjFowjHxcRP+yvLUMdpwF/TzW8pXSLiBiduY6D2Hkm1TMZ+76L6niFp9PyNKpvpP9ItZHyngw19Jztt5Pcs/0kvYZqskPjGIHvA5+OiF8MZT8duaWfwn2apH2oPtg297fOMHlS0n+jGuKBauz0qcw1dEu6qlbDe6lmC2Qh6Z1UB8aNacwYSkYDW3PVkXwJ6Pkh06xtuP0DcEpEZJsWWCfpFOALwB8BG4FDqYY6jsxYxkG1wD+ZKuxPiYj/J+mcTDU0ZvudBryWamgJqnnyazPVUHc18AjQ+MA7k2rWW59j/ruqI0O/5yempGH5xByADwCXsmP44F4yfXWtOZfqq/sFVFuU9wI5DzFfTzVm+i52/rDZDPxVjgIkHUs1pNbVmC6bjKY6MCq351sV+Ml/B6YBd0fE0ZJOoAq6nLakmWXjqH43j46I5ySNBrKcA6exM1nSZyKifgTuLZJy7/MCODwi/ktt+VJJK4a6k44MfTJ9YvZG0t5Uc39fD6wEPtaiGTNExBaqrbocxwQ06/8h4CFJX6P6fTu0sX8ho1cB+6T+9621/5Lq0P/cuiV9neognC2Nxow7lH8XES9I2kPSHhFxj6S/z9R3w3uBecBvqYa6FqagnUn+0zB0SXpdRDwJ26d8D/3pD/r3a0lvjYj7Uh3HAb8e6k46dUx/RURM7q9tGPv/OtVJrX4AvBNYGxEX5ei7VkPTaYENLRjTP4Xq6/SrImKCpMlU376yjZtKOqwNdqIi6ZomzRERWc7LJOlu4FTgMqpTg2wE3hwROScY9KzpaOBPqU6FcHfmvmdQTbp4MjWNB86JiO9kruMoqtMvvCY1/RswJyIeHtJ+OjT0lwEf7/GJ+bmIODZT//UDkPYElufcYZn6PazxkGoq3E4nnMsdfpIeoJo1872IODq1Zd2hnGYu/TXVH/X2b7kR8bbe1ulEkl5NdVoOUe1nGg1cP9SH+++OdBDdrIi4PnO/I4F/lxYfT9+Qc/V9aH0nehriIiJ+ORz9derwzrlUXxdfQ/WL/TPynltk+1BOVBeDz9j19n63h7qkLW2whbs1In7Rin+Lmpuopit+hXTul1ZIpzz4EnAc1bex+4ALe5waYjj63czLv/01/kM+mebK/21ELB3OOlIto6n2NY2hOv/RkrT8cWAF1ey7nI5hx8bAUarO1XVdpr6/TZpMIOkbPcb1h1xHhn5ErKD6j2tMgXsJOIPq7IY5HCWp8SktYFRabsnUvDbxiKQ/B0aoOvnaBcD/zVzD1oi4MnOfzVxDdWrh09Py+1Lb24ez04jYt7fn0hb2kVRhm2MWz1ephi+WUZ1k7ONU+15mpr/fbCR9FTic6sOmsTEQVEMtWUqoPR72ExB2VOj32Hq4Gbg7Lf818BCZth4iohUzQnainS+WMiqNmW7/5YqIBzOXdD7wt1Q7Lm+gOtPkZzLXcIuqi5V8i513oOYe1uiKiPq4/rWSLspcw04iYhvVDvcvZerydbUh0K9QHTx5aIumV08BJkXrxrqjl8fDoqPG9FWdzbKx9XAi1aHlr6L66ryihaVlJ+mePp6O0saxASQ1O0YiIuPpnVMdd1OdauCG1DQbeH9EnJizjlbqeWBe7gP1etRyE3BB44jpFvS/jeq0IKI6p/5LjacYhpGBTgv9+g7UEbR268EA9XNJxNxHPbYDSYcC/0R1oZ+gGua6sA32u2RTCzrYOeyyD4GmDaTJVBdcqn8D7MjfzY4a3mHnHajbJD3lwN9B0oKImJu522OpToNxA9V5dlp5bdqmJ3jLuMOusTHy2U4NlIFqhyHQmk+1uoCcOi30vQO1b1Na0OdrqXZQzgb+nGr66A2R8UyjNfVTa+9NNQT4IPl22DU2RrokvSpqZ4K11onCzt3fUaHfZlsP7Whj7g7TDsI7gTvTXOjZwPckfToicu00bNRyfn05Ten9ai8vH05rgR+moa/6KZ5bctR0qXqZwgodvpHYUaFvvUszm07v94XD0/dIqku/zaaaC305mU5n3I+XgIkt6Hd9uu3BjtNCdM7OtVeIvqawdjKHfoeTNIVqDvi+afkXwAciIsuZNiUtpJr3fQdwaUQ8kqPfXmqpn0p3BPDHwKIWlPJoRNxUb1B1bQGzYddRs3fs5SQ9DJwXET9Iy28Frsh1+gNJv2fHEEb9l60VszTq12XdCjw93EfB9lJHy68tYOXyln7n29wIfICIuC+NZWYREXvk6qs/EfF9SQezY4fuEzn7b7NrC1ih2uYP0obNckn/R9Lxkv5E0hVUO1Lf1OOo3Y4n6T1Uc7FPpzrt9v2Scp5auXFtgd9QXVugcVsMTM9YhxXMwzsdzkfm7iDpIeDtEbExLXdRXUgk2+UrU7+jgV+lmU2NufsjI+Klvtc0GzwP73S4iDih1TW0kT0agZ+8QGu+7d5Fde74F9PyqNTWsvPZWzkc+h1K0vsi4l+08+UBtyt0Tvidkr7DjnPenAHc3oI69o6IRuATES+mc9ybDTuHfudqXGe0yLnIdZJeDxwcER+XdBrwVqrZQ8vIf952gF9JelPjTKeSjmEYLotn1ozH9K3jSboV+ETPy86lYxguiYhTMtfzZuBGqh27AIcAZ+Q6dsLK5tDvUD2mBL5MRFyQq5ZWk/RIRDS9MEj9zKyZa9oLeCPVN47HI+J3/axiNiQ8vNO56luNlwKXtKqQNrB3H8+NylZFksbvPwocFhEfkjRR0hsj4tbctVh5vKVfAEn/2rgYeYkk3QB8NyK+3KP9bOAdEXFG5nq+TvWhfFZEHClpFLAsIibnrMPK5C39MpT+yX4R8C1J72XHN6ApVFdVe3cL6jk8Is6QNBsgIn6tFl8x3srh0LeOFxHPA2+RdAI7Lvp9W0R8t0Ul/TZt3QeApMOpXbHJbDh5eKdD9ThX+KsZ5utu2sBJejvwd8AkqoOyjgP+IiK+18q6rAwOfbMWkHQAMI3qQ/hHEfHTFpdkhXDom2XS3wnuGgdrmQ0nh75ZJj75nbUDh76ZWUF8Pn2zTCT9Te3x6T2e+2z+iqxEDn2zfGbVHl/c47kZOQuxcjn0zfJRL4+bLZsNC4e+WT7Ry+Nmy2bDwjtyzTKRtA34FdVW/Sh2PmBu74jYq1W1WTkc+mZmBfHwjplZQRz6ZmYFceibmRXEoW9mVhCHvplZQf4/YpkijGnx6AMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eng_train_data['Genre'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e64303",
   "metadata": {},
   "source": [
    "As can be oberved on the previous graphic, we cannot say that the data is umblanced. Because the porpose of this project is to find good methods to find the musical genre (but the accuracy of the models is not so important) we will not use balancing data techniques.\n",
    "\n",
    "Next, we will apply word embedding for the resulted dataset that contains only english lycrics in order to see if we will obtain better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f0cc9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text_for_wemb(eng_train_data)\n",
    "preprocess_text_for_wemb(eng_test_data)\n",
    "\n",
    "\n",
    "X_eng_wemb_train = []\n",
    "for index, row in eng_train_data.iterrows():\n",
    "    X_eng_wemb_train.append(create_vects(row['wemb_lyrics']))\n",
    "\n",
    "X_eng_wemb_test = []\n",
    "for index, row in eng_test_data.iterrows():\n",
    "    X_eng_wemb_test.append(create_vects(row['wemb_lyrics']))\n",
    "\n",
    "Y_eng_wemb_train = eng_train_data['Genre']\n",
    "Y_eng_wemb_test = eng_test_data['Genre']\n",
    "\n",
    "#split the training dataset into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_eng_wemb_train, X_eng_wemb_valid, Y_eng_wemb_train, Y_wemb_valid = train_test_split(X_eng_wemb_train, Y_eng_wemb_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b2c0d4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only English lyrics and Word Embeddings with SVM accuracy score on the validation dataset:  0.4072972972972973\n",
      "Only English lyrics and Word Embeddings with SVM accuracy score on the test dataset:  0.39714970361962415\n",
      "Only English lyrics and Word Embeddings with Random Forest accuracy score on the validation dataset:  0.3645945945945946\n",
      "Only English lyrics and Word Embeddings with Random Forest accuracy score on the test dataset:  0.3618362971370917\n",
      "Only English lyrics and Word Embeddings with Naive Bayes accuracy score on the validation dataset:  0.2997297297297297\n",
      "Only English lyrics and Word Embeddings with Naive Bayes accuracy score on the test dataset:  0.2995333585571951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raradu\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only English lyrics and Word Embeddings with Neural Networks accuracy score on the validation dataset:  0.2997297297297297\n",
      "Only English lyrics and Word Embeddings with Neural Networks accuracy score on the test dataset:  0.2995333585571951\n"
     ]
    }
   ],
   "source": [
    "run_models(X_eng_wemb_train, Y_eng_wemb_train, X_eng_wemb_valid, Y_wemb_valid, X_eng_wemb_test, Y_eng_wemb_test, \"Only English lyrics and Word Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b59c60",
   "metadata": {},
   "source": [
    "<b>Observation: </b> It seems that the word embeddings technique applied only for the lycrics of english songs is not an improvement; the obtained accuracy is a little lower than the previous one.\n",
    "\n",
    "In the above table, it can be observed all the obtained results for the word embeddings method applied only for the english songs:\n",
    "\n",
    "| No.| Model |Validation Dataset Accuracy | Test Dataset Accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| 1. | SVM | 0.4072 | 0.3971 |\n",
    "| 2. | Random Forest |0.3645 | 0.3618 |\n",
    "| 3. | Naive Bayes | 0.2997 | 0.2995 |\n",
    "| 4. | Neural Network | 0.2997 | 0.2995 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221694e",
   "metadata": {},
   "source": [
    "<h3>The stopwords</h3>\n",
    "\n",
    "Next we will use the stopwords in the word embedding technique. For some musical genre, the stopwords can be important as it can be a defining feature of that musical genre (i.e. for Rap music, the artists are using more stopwords that in Folk). For a better consistency we will continue to use only the dataset that contains only english songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "349ef787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    all_lyrics = []\n",
    "    for lyric in data['tknz_lyrics']:\n",
    "        current_lyric = []\n",
    "        for word in lyric:\n",
    "            if word.isalpha():\n",
    "                current_lyric.append(word.lower())\n",
    "        all_lyrics.append(current_lyric)\n",
    "    return all_lyrics\n",
    "\n",
    "def remove_stopwords(all_lyrics):\n",
    "    stop_words_eng = set(stopwords.words('english'))\n",
    "    without_stop_words = []\n",
    "    for lyric in all_lyrics:\n",
    "        current_lyric = []\n",
    "        for word in lyric:\n",
    "            if word not in stop_words_eng:\n",
    "                current_lyric.append(word)\n",
    "        without_stop_words.append(current_lyric)\n",
    "    return without_stop_words\n",
    "\n",
    "def apply_steammer(all_lyrics):\n",
    "    snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    stemmed_lyrics = []\n",
    "    for lyric in all_lyrics:\n",
    "        aux_lyric = []\n",
    "        for word in lyric:\n",
    "            aux_lyric.append(snowball_stemmer.stem(str(word)))\n",
    "        stemmed_lyrics.append(aux_lyric)\n",
    "    return stemmed_lyrics\n",
    "\n",
    "def preprocess_text_for_wemb_with_stopwords(data):\n",
    "    without_punctuation = remove_punctuation(data)\n",
    "    stemmer_applied = apply_steammer(without_punctuation)\n",
    "    \n",
    "    detokenized_lyrics = []\n",
    "    for crnt in stemmer_applied:\n",
    "        detokenized_lyrics.append(TreebankWordDetokenizer().detokenize(crnt))\n",
    "        \n",
    "    data['wemb_lyrics_with_stopwords'] = detokenized_lyrics\n",
    "\n",
    "preprocess_text_for_wemb_with_stopwords(eng_train_data)\n",
    "preprocess_text_for_wemb_with_stopwords(eng_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "104b1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eng_stp_wemb_train = []\n",
    "for index, row in eng_train_data.iterrows():\n",
    "    X_eng_stp_wemb_train.append(create_vects(row['wemb_lyrics_with_stopwords']))\n",
    "\n",
    "X_eng_stp_wemb_test = []\n",
    "for index, row in eng_test_data.iterrows():\n",
    "    X_eng_stp_wemb_test.append(create_vects(row['wemb_lyrics_with_stopwords']))\n",
    "\n",
    "Y_eng_stp_wemb_train = eng_train_data['Genre']\n",
    "Y_eng_stp_wemb_test = eng_test_data['Genre']\n",
    "\n",
    "#split the training dataset into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_eng_stp_wemb_train, X_eng_stp_wemb_valid, Y_eng_stp_wemb_train, Y_eng_stp_wemb_valid = train_test_split(X_eng_stp_wemb_train, Y_eng_stp_wemb_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d9efe6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric with stopwords and Word Embeddings with SVM accuracy score on the validation dataset:  0.3908108108108108\n",
      "Lyric with stopwords and Word Embeddings with SVM accuracy score on the test dataset:  0.39525791398663135\n",
      "Lyric with stopwords and Word Embeddings with Random Forest accuracy score on the validation dataset:  0.34324324324324323\n",
      "Lyric with stopwords and Word Embeddings with Random Forest accuracy score on the test dataset:  0.3527557068987262\n",
      "Lyric with stopwords and Word Embeddings with Naive Bayes accuracy score on the validation dataset:  0.2691891891891892\n",
      "Lyric with stopwords and Word Embeddings with Naive Bayes accuracy score on the test dataset:  0.26913860512044396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raradu\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric with stopwords and Word Embeddings with Neural Networks accuracy score on the validation dataset:  0.2691891891891892\n",
      "Lyric with stopwords and Word Embeddings with Neural Networks accuracy score on the test dataset:  0.26913860512044396\n"
     ]
    }
   ],
   "source": [
    "run_models(X_eng_stp_wemb_train, Y_eng_stp_wemb_train, X_eng_stp_wemb_valid, Y_eng_stp_wemb_valid, X_eng_stp_wemb_test, Y_eng_stp_wemb_test, \"Lyric with stopwords and Word Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68eb3cf",
   "metadata": {},
   "source": [
    "<b>Observation: </b> It seems that the word embeddings technique applied for the lycrics of english songs with the stopwords is worst than the previous trying.\n",
    "\n",
    "In the above table, it can be observed all the obtained results for the word embeddings method applied only for the english songs with stopwords:\n",
    "\n",
    "| No.| Model |Validation Dataset Accuracy | Test Dataset Accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| 1. | SVM | 0.3908 | 0.3952 |\n",
    "| 2. | Random Forest | 0.3432 | 0.3527 |\n",
    "| 3. | Naive Bayes | 0.2691 | 0.2691 |\n",
    "| 4. | Neural Network | 0.2691 | 0.2691 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8709fd",
   "metadata": {},
   "source": [
    "<h3>The punctuation</h3>\n",
    "\n",
    "We will try next to run the models on the english songs dataset using also the punctuation. In some music genre the punctuation can be used more as it can express the feelings of the artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f6da61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_for_wemb_with_stopwords_and_punctuation(data):\n",
    "    stemmer_applied = apply_steammer(data['tknz_lyrics'])\n",
    "    \n",
    "    detokenized_lyrics = []\n",
    "    for crnt in stemmer_applied:\n",
    "        detokenized_lyrics.append(TreebankWordDetokenizer().detokenize(crnt))\n",
    "        \n",
    "    data['wemb_lyrics_with_stopwords_punctuation'] = detokenized_lyrics\n",
    "\n",
    "preprocess_text_for_wemb_with_stopwords_and_punctuation(eng_train_data)\n",
    "preprocess_text_for_wemb_with_stopwords_and_punctuation(eng_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "699ed7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>tknz_lyrics</th>\n",
       "      <th>wemb_lyrics</th>\n",
       "      <th>wemb_lyrics_with_stopwords</th>\n",
       "      <th>wemb_lyrics_with_stopwords_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metal</td>\n",
       "      <td>I am a night in to the darkness, only soul los...</td>\n",
       "      <td>[I, am, a, night, in, to, the, darkness, ,, on...</td>\n",
       "      <td>night dark soul lost walk throught night fores...</td>\n",
       "      <td>i am a night in to the dark onli soul lost wit...</td>\n",
       "      <td>i am a night in to the dark, onli soul lost wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>Yeah\\nSometimes, i just wanna fly away.\\nThey ...</td>\n",
       "      <td>[Yeah, Sometimes, ,, i, just, wan, na, fly, aw...</td>\n",
       "      <td>yeah sometim wannafli away say ca ca skate ca ...</td>\n",
       "      <td>yeah sometim i just wannafli away they say i c...</td>\n",
       "      <td>yeah sometim, i just wannafli away . they say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metal</td>\n",
       "      <td>Do you work hard?\\nDo you work hard?\\nYou don'...</td>\n",
       "      <td>[Do, you, work, hard, ?, Do, you, work, hard, ...</td>\n",
       "      <td>work hard work hard chicago work hard tri hard...</td>\n",
       "      <td>do you work hard do you work hard you do you d...</td>\n",
       "      <td>do you work hard? do you work hard? you don't!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>You know what? I'm destined to be the last man...</td>\n",
       "      <td>[You, know, what, ?, I, 'm, destined, to, be, ...</td>\n",
       "      <td>know destin last man standin care mappin escap...</td>\n",
       "      <td>you know what i destin to be the last man stan...</td>\n",
       "      <td>you know what? i'm destin to be the last man s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Genre                                             Lyrics  \\\n",
       "0    Metal  I am a night in to the darkness, only soul los...   \n",
       "1  Hip-Hop  Yeah\\nSometimes, i just wanna fly away.\\nThey ...   \n",
       "2    Metal  Do you work hard?\\nDo you work hard?\\nYou don'...   \n",
       "3  Hip-Hop  You know what? I'm destined to be the last man...   \n",
       "\n",
       "                                         tknz_lyrics  \\\n",
       "0  [I, am, a, night, in, to, the, darkness, ,, on...   \n",
       "1  [Yeah, Sometimes, ,, i, just, wan, na, fly, aw...   \n",
       "2  [Do, you, work, hard, ?, Do, you, work, hard, ...   \n",
       "3  [You, know, what, ?, I, 'm, destined, to, be, ...   \n",
       "\n",
       "                                         wemb_lyrics  \\\n",
       "0  night dark soul lost walk throught night fores...   \n",
       "1  yeah sometim wannafli away say ca ca skate ca ...   \n",
       "2  work hard work hard chicago work hard tri hard...   \n",
       "3  know destin last man standin care mappin escap...   \n",
       "\n",
       "                          wemb_lyrics_with_stopwords  \\\n",
       "0  i am a night in to the dark onli soul lost wit...   \n",
       "1  yeah sometim i just wannafli away they say i c...   \n",
       "2  do you work hard do you work hard you do you d...   \n",
       "3  you know what i destin to be the last man stan...   \n",
       "\n",
       "              wemb_lyrics_with_stopwords_punctuation  \n",
       "0  i am a night in to the dark, onli soul lost wi...  \n",
       "1  yeah sometim, i just wannafli away . they say ...  \n",
       "2  do you work hard? do you work hard? you don't!...  \n",
       "3  you know what? i'm destin to be the last man s...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_train_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b524d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eng_stp_pct_wemb_train = []\n",
    "for index, row in eng_train_data.iterrows():\n",
    "    X_eng_stp_pct_wemb_train.append(create_vects(row['wemb_lyrics_with_stopwords_punctuation']))\n",
    "\n",
    "X_eng_stp_pct_wemb_test = []\n",
    "for index, row in eng_test_data.iterrows():\n",
    "    X_eng_stp_pct_wemb_test.append(create_vects(row['wemb_lyrics_with_stopwords_punctuation']))\n",
    "\n",
    "Y_eng_stp_pct_wemb_train = eng_train_data['Genre']\n",
    "Y_eng_stp_pct_wemb_test = eng_test_data['Genre']\n",
    "\n",
    "#split the training dataset into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_eng_stp_pct_wemb_train, X_eng_stp_pct_wemb_valid, Y_eng_stp_pct_wemb_train, Y_eng_stp_pct_wemb_valid = train_test_split(X_eng_stp_pct_wemb_train, Y_eng_stp_pct_wemb_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "88c118b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric with stopwords and punctuation and Word Embeddings with SVM accuracy score on the validation dataset:  0.4002702702702703\n",
      "Lyric with stopwords and punctuation and Word Embeddings with SVM accuracy score on the test dataset:  0.4078698448732501\n",
      "Lyric with stopwords and punctuation and Word Embeddings with Random Forest accuracy score on the validation dataset:  0.3564864864864865\n",
      "Lyric with stopwords and punctuation and Word Embeddings with Random Forest accuracy score on the test dataset:  0.3589355530331694\n",
      "Lyric with stopwords and punctuation and Word Embeddings with Naive Bayes accuracy score on the validation dataset:  0.27918918918918917\n",
      "Lyric with stopwords and punctuation and Word Embeddings with Naive Bayes accuracy score on the test dataset:  0.2813721780804641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raradu\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric with stopwords and punctuation and Word Embeddings with Neural Networks accuracy score on the validation dataset:  0.27918918918918917\n",
      "Lyric with stopwords and punctuation and Word Embeddings with Neural Networks accuracy score on the test dataset:  0.2813721780804641\n"
     ]
    }
   ],
   "source": [
    "run_models(X_eng_stp_pct_wemb_train, Y_eng_stp_pct_wemb_train, X_eng_stp_pct_wemb_valid, Y_eng_stp_pct_wemb_valid, X_eng_stp_pct_wemb_test, Y_eng_stp_pct_wemb_test, \"Lyric with stopwords and punctuation and Word Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019f994",
   "metadata": {},
   "source": [
    "<b>Observation: </b> It seems that if we are adding the punctuations we are obtaining better results than the previous trying when we added the stopwords to the word embedding.\n",
    "\n",
    "In the above table, it can be observed all the obtained results for the word embeddings method applied only for the english songs with stopwords and punctuation:\n",
    "\n",
    "| No.| Model |Validation Dataset Accuracy | Test Dataset Accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| 1. | SVM | 0.4002 | 0.4078 |\n",
    "| 2. | Random Forest | 0.3564 | 0.3589 |\n",
    "| 3. | Naive Bayes | 0.2791 | 0.2813 |\n",
    "| 4. | Neural Network | 0.2791 | 0.2813 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c7ebfe",
   "metadata": {},
   "source": [
    "<h3>Steamming process</h3>\n",
    "\n",
    "As the results obtained for the case in which we don't remove stopwords and punctuation we will next try to remove them but not perform the steamming process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4b9c5adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_for_wemb_without_steamming(data):\n",
    "     detokenized_lyrics = []\n",
    "    for crnt in data['Lyrics']:\n",
    "        detokenized_lyrics.append(TreebankWordDetokenizer().detokenize(crnt))\n",
    "\n",
    "    data['wemb_lyrics_without_steamming'] = detokenized_lyrics\n",
    "\n",
    "preprocess_text_for_wemb_without_steamming(eng_train_data)\n",
    "preprocess_text_for_wemb_without_steamming(eng_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5d866c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eng_stm_wemb_train = []\n",
    "for index, row in eng_train_data.iterrows():\n",
    "    X_eng_stm_wemb_train.append(create_vects(row['wemb_lyrics_without_steamming']))\n",
    "\n",
    "X_eng_stm_wemb_test = []\n",
    "for index, row in eng_test_data.iterrows():\n",
    "    X_eng_stm_wemb_test.append(create_vects(row['wemb_lyrics_without_steamming']))\n",
    "\n",
    "Y_eng_stm_wemb_train = eng_train_data['Genre']\n",
    "Y_eng_stm_wemb_test = eng_test_data['Genre']\n",
    "\n",
    "#split the training dataset into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_eng_stm_wemb_train, X_eng_stm_wemb_valid, Y_eng_stm_wemb_train, Y_eng_stm_wemb_valid = train_test_split(X_eng_stm_wemb_train, Y_eng_stm_wemb_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "47c7f9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric without steamming and Word Embeddings with SVM accuracy score on the validation dataset:  0.4245945945945946\n",
      "Lyric without steamming and Word Embeddings with SVM accuracy score on the test dataset:  0.41341909446336234\n",
      "Lyric without steamming and Word Embeddings with Random Forest accuracy score on the validation dataset:  0.37405405405405406\n",
      "Lyric without steamming and Word Embeddings with Random Forest accuracy score on the test dataset:  0.37406987009711185\n",
      "Lyric without steamming and Word Embeddings with Naive Bayes accuracy score on the validation dataset:  0.3108108108108108\n",
      "Lyric without steamming and Word Embeddings with Naive Bayes accuracy score on the test dataset:  0.30911842603102535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raradu\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric without steamming and Word Embeddings with Neural Networks accuracy score on the validation dataset:  0.3108108108108108\n",
      "Lyric without steamming and Word Embeddings with Neural Networks accuracy score on the test dataset:  0.30911842603102535\n"
     ]
    }
   ],
   "source": [
    "run_models(X_eng_stm_wemb_train, Y_eng_stm_wemb_train, X_eng_stm_wemb_valid, Y_eng_stm_wemb_valid, X_eng_stm_wemb_test, Y_eng_stm_wemb_test, \"Lyric without steamming and Word Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac739f",
   "metadata": {},
   "source": [
    "<b>Observation: </b> It seems that if we are removing the steamming process we are obtaining better than on the previous trying:\n",
    "\n",
    "| No.| Model |Validation Dataset Accuracy | Test Dataset Accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| 1. | SVM | 0.4245 | 0.4134 |\n",
    "| 2. | Random Forest | 0.3740 | 0.3740 |\n",
    "| 3. | Naive Bayes | 0.3108 | 0.2996 |\n",
    "| 4. | Neural Network | 0.3005 | 0.3091 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31ddaa",
   "metadata": {},
   "source": [
    "<h3>Adding new features to the dataset</h3>\n",
    "\n",
    "We will try to add new information to the dataset. For some music genre, the punctuation can still be a good indicator of the genre so we will add a feature that represents the punctuation counts from lyric.\n",
    "\n",
    "A new possible feature could be the number of words per line, as in some music genre the artists have a good word speed per lines (i.e. rap, pop) than for others (i.e. folk, jazz). The words per line will be defined as the number of words divided by the number of lines.\n",
    "\n",
    "These new features will be added to the coumn were the lyrics were added with stopwords, punctuation and without the performing of the steamming process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d6845c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation(data):\n",
    "    pct_count = 0\n",
    "    pct_count_list = []\n",
    "    count = lambda chr1, chr2: len(list(filter(lambda ch: ch in chr2, chr1)))\n",
    "    for crnt in data['Lyrics']:\n",
    "        pct_count = count(crnt, string.punctuation)\n",
    "        pct_count_list.append(pct_count)\n",
    "        pct_count = 0\n",
    "    data['punctuation_count'] = pct_count_list\n",
    "\n",
    "count_punctuation(eng_train_data)\n",
    "count_punctuation(eng_test_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "29909076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_per_line(data):\n",
    "    lines_count = 0\n",
    "    words_count = 0\n",
    "    words_per_line = 0\n",
    "    words_per_line_list = []\n",
    "    for crnt in data['Lyrics']:\n",
    "        words_count = sum([i.strip(string.punctuation).isalpha() for i in crnt.split()])\n",
    "        lines_count = crnt.count('\\n')\n",
    "        if (lines_count != 0):\n",
    "            words_per_line = round(words_count / lines_count)\n",
    "        else:\n",
    "            words_per_line = round(words_count / 1)\n",
    "        words_per_line_list.append(words_per_line)\n",
    "        lines_count = 0\n",
    "        words_count = 0\n",
    "        words_per_line = 0\n",
    "    data['words_per_lines'] = words_per_line_list\n",
    "count_words_per_line(eng_train_data)\n",
    "count_words_per_line(eng_test_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "56baa2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eng_wemb_train = []\n",
    "for index, row in eng_train_data.iterrows():\n",
    "    X_eng_wemb_train.append(create_vects(row['wemb_lyrics_without_steamming']))\n",
    "\n",
    "X_eng_wemb_test = []\n",
    "for index, row in eng_test_data.iterrows():\n",
    "    X_eng_wemb_test.append(create_vects(row['wemb_lyrics_without_steamming']))\n",
    "    \n",
    "Y_eng_wemb_train = eng_train_data['Genre']\n",
    "Y_eng_wemb_test = eng_test_data['Genre']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d4b7f45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18498 entries, 0 to 18497\n",
      "Columns: 302 entries, 0 to words_per_lines\n",
      "dtypes: float32(300), int64(2)\n",
      "memory usage: 21.5 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7929 entries, 0 to 7928\n",
      "Columns: 302 entries, 0 to words_per_lines\n",
      "dtypes: float32(300), int64(2)\n",
      "memory usage: 9.2 MB\n"
     ]
    }
   ],
   "source": [
    "X_data_train = pd.DataFrame(X_eng_wemb_train)\n",
    "X_data_test = pd.DataFrame(X_eng_wemb_test)\n",
    "\n",
    "X_data_train['punctuation_count'] = eng_train_data['punctuation_count']\n",
    "X_data_test['punctuation_count'] = eng_test_data['punctuation_count']\n",
    "\n",
    "X_data_train['words_per_lines'] = eng_train_data['words_per_lines']\n",
    "X_data_test['words_per_lines'] = eng_test_data['words_per_lines']\n",
    "\n",
    "X_data_train.info()\n",
    "X_data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "125cce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the training dataset into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_data_train, X_data_valid, Y_eng_wemb_train, Y_data_valid = train_test_split(X_data_train, Y_eng_wemb_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "52ba5b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Words Per Line and Punctuation Count features and Word Embeddings with SVM accuracy score on the validation dataset:  0.4254054054054054\n",
      "Added Words Per Line and Punctuation Count features and Word Embeddings with SVM accuracy score on the test dataset:  0.416097890074354\n",
      "Added Words Per Line and Punctuation Count features and Word Embeddings with Random Forest accuracy score on the validation dataset:  0.38134486467886487\n",
      "Added Words Per Line and Punctuation Count features and Word Embeddings with Random Forest accuracy score on the test dataset:  0.38507709456836537\n",
      "Added Words Per Line and Punctuation Count features and Word Embeddings with Naive Bayes accuracy score on the validation dataset:  0.31548743243765245\n",
      "Added Words Per Line and Punctuation Count features and Word Embeddings with Naive Bayes accuracy score on the test dataset:  0.3106623423369908\n",
      "Added Words Per Line and Punctuation Count features and Word Embeddings with Neural Networks accuracy score on the validation dataset:  0.32343243243243245\n",
      "Added Words Per Line and Punctuation Count features and Word Embeddings with Neural Networks accuracy score on the test dataset:  0.3678684323369908\n"
     ]
    }
   ],
   "source": [
    "run_models(X_data_train, Y_eng_wemb_train, X_data_valid, Y_data_valid, X_data_test, Y_eng_wemb_test, \"Added Words Per Line and Punctuation Count features and Word Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e0e89",
   "metadata": {},
   "source": [
    "<b>Observation: </b> It seems that these new featres are not a great improvement to the model, but still performs a little better than the previous trying:\n",
    "\n",
    "| No.| Model |Validation Dataset Accuracy | Test Dataset Accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| 1. | SVM | 0.4254 | 0.4160 |\n",
    "| 2. | Random Forest | 0.3813 | 0.3154 |\n",
    "| 3. | Naive Bayes | 0.3154 | 0.3106 |\n",
    "| 4. | Neural Network | 0.3234 | 0.3678 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620b5648",
   "metadata": {},
   "source": [
    "<h3>Conclusions</h3>\n",
    "\n",
    "Regarding all the NLP used techniques, the word embeddings technique was better than the bag of words for the given dataset.\n",
    "\n",
    "In this project, we used 4 machine learning models: SVM, Random Forest, Naive Bayes, and Neural Networks. The hiper parameters were tuned after some trying, but not with the best possible values as the accuracy of the models was not the scope of this project. It was observed that the SVM model performs better than the others for the given dataset.\n",
    "\n",
    "Regarding the preprocessing step, an interesting fact is that we obtained better results if we don't apply the classical text preprocessing steps (removal of stopwords and punctuation and the process of steamming).  \n",
    "\n",
    "In the end, we added two additional features to the dataset: Punctuation count and Words per line. Because we are trying to classify music genres, these two features can be decisive in distinguishing between different music types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d51c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
